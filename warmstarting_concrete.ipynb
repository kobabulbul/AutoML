{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import torch.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.model_selection\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "from joblib import Parallel, delayed\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create MLP from literature\n",
    "class ConcreteDataset(Dataset):\n",
    "  def __init__(self, data, labels):\n",
    "    # takes input data and target labels with dtype numpy array and converts it to a FloatTensor\n",
    "    self.data = data# Input Data\n",
    "    self.labels = labels # Target Labels\n",
    "  def __len__(self):\n",
    "    # returns size of dataset\n",
    "    return len(self.labels)\n",
    "  def __getitem__(self, idx):\n",
    "    # returns a single data row with target label\n",
    "    X = self.data[idx, :]\n",
    "    y = self.labels[idx].view(-1)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "class ConcreteNN(nn.Module):\n",
    "\n",
    "  def __init__(self, size):\n",
    "    super().__init__()\n",
    "    # layers\n",
    "    self.input_layer = nn.Linear(size, size)     \n",
    "    self.hidden_layer = nn.Linear(size, size)\n",
    "    self.output_layer = nn.Linear(size, 1) \n",
    "    self.activation = nn.ReLU()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.input_layer(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.hidden_layer(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.output_layer(x)\n",
    "    return x\n",
    "\n",
    "def train(model, X_train, y_train, X_test, y_test):\n",
    "  criterion = nn.MSELoss()\n",
    "  #optimizer= torch.optim.SGD(model.parameters(), lr=0.001, momentum= 0.5)\n",
    "  optimizer= torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "  epochs = 25000\n",
    "  loss_over_time = []\n",
    "  test_loss_over_time = []\n",
    "  training_data = ConcreteDataset(X_train, y_train)\n",
    "  train_dataloader = DataLoader(training_data, batch_size = 128, shuffle=True)\n",
    "  for i in range(epochs):\n",
    "    for X_train, y_train in train_dataloader:\n",
    "      model.train()\n",
    "      optimizer.zero_grad()\n",
    "      output = model(X_train).flatten()\n",
    "      target = y_train.flatten()\n",
    "      loss = criterion(output, target)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    loss_over_time.append(loss.item())\n",
    "    test_loss_over_time.append(test_loss(model=model, X_test=X_test, y_test=y_test))\n",
    "  return loss_over_time, test_loss_over_time\n",
    "  \n",
    "def test_loss(model, X_test, y_test):\n",
    "  model.eval()\n",
    "  output = model(X_test)\n",
    "  loss = sklearn.metrics.mean_squared_error(output.detach().numpy(), y_test.detach().numpy())\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model(X_train, y_train):\n",
    "    LR = sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
    "    inputs_reg = X_train[:, [0, 3, 5, 6]]\n",
    "    LR.fit(inputs_reg, y_train)\n",
    "    return torch.FloatTensor(LR.predict(inputs_reg))\n",
    "\n",
    "def train_cv(train_index, test_index):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model = ConcreteNN(9)\n",
    "\n",
    "    # warmstarting (shrinking and perturb): https://arxiv.org/pdf/1910.08475.pdf\n",
    "    # Most significantly, it allows us to\n",
    "    # quickly fit high-performing models in sequential environments without having to retrain from scratch.\n",
    "    # Separately, it offers a slight regularization benefit, which in combination with the first property\n",
    "    # sometimes allows shrink-perturb models to generalize even better than randomly-initialized models.\n",
    "\n",
    "    lamb = 0.3\n",
    "    sigma = 0.001\n",
    "    with torch.no_grad():\n",
    "        for i in range(7):\n",
    "            for j in range(7):\n",
    "                model.input_layer.weight[i][j] = start_model.input_layer.weight[i][j]*lamb  + torch.normal(0.0, sigma, size=(1,1))\n",
    "                model.hidden_layer.weight[i][j] = start_model.hidden_layer.weight[i][j]*lamb  + torch.normal(0.0, sigma, size=(1,1))\n",
    "            model.output_layer.weight[0][i] = start_model.output_layer.weight[0][i]*lamb  + torch.normal(0.0, sigma, size=(1,1))\n",
    "            model.input_layer.bias[i] = start_model.input_layer.bias[i]*lamb  + torch.normal(0.0, sigma, size=(1,1))\n",
    "            model.hidden_layer.bias[i] = start_model.hidden_layer.bias[i]*lamb  + torch.normal(0.0, sigma, size=(1,1))\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer= torch.optim.SGD(model.parameters(), lr=0.001, momentum= 0.5)\n",
    "    epochs = 2000\n",
    "    loss_over_time = []\n",
    "    test_loss_over_time = []\n",
    "\n",
    "    # Training Loop\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train).flatten()\n",
    "        loss = criterion(y_train, output)\n",
    "        loss_over_time.append(loss.item())\n",
    "        test_loss_over_time.append(test_loss(model=model, X_test=X_test, y_test=y_test))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return test_loss_over_time[-1]\n",
    "\n",
    "def train_multirun(K, i):\n",
    "    skf = sklearn.model_selection.KFold(n_splits=K)\n",
    "    skf.get_n_splits(X, y)\n",
    "    loss_test = []\n",
    "    cv_loss_test = Parallel(n_jobs=-1)(delayed(train_cv)(train_index, test_index) for train_index, test_index in skf.split(X, y))\n",
    "    return cv_loss_test\n",
    "\n",
    "def train_iml_cv(train_index, test_index):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model = ConcreteNN(9)\n",
    "\n",
    "    # warmstarting (shrinking and perturb): https://arxiv.org/pdf/1910.08475.pdf\n",
    "    # Most significantly, it allows us to\n",
    "    # quickly fit high-performing models in sequential environments without having to retrain from scratch.\n",
    "    # Separately, it offers a slight regularization benefit, which in combination with the first property\n",
    "    # sometimes allows shrink-perturb models to generalize even better than randomly-initialized models.\n",
    "\n",
    "    lamb = 0.3\n",
    "    sigma = 0.001\n",
    "    with torch.no_grad():\n",
    "        for i in range(7):\n",
    "            for j in range(7):\n",
    "                model.input_layer.weight[i][j] = start_model.input_layer.weight[i][j]*lamb  + torch.normal(0.0, sigma, size=(1,1))\n",
    "                model.hidden_layer.weight[i][j] = start_model.hidden_layer.weight[i][j]*lamb  + torch.normal(0.0, sigma, size=(1,1))\n",
    "            model.output_layer.weight[0][i] = start_model.output_layer.weight[0][i]*lamb  + torch.normal(0.0, sigma, size=(1,1))\n",
    "            model.input_layer.bias[i] = start_model.input_layer.bias[i]*lamb  + torch.normal(0.0, sigma, size=(1,1))\n",
    "            model.hidden_layer.bias[i] = start_model.hidden_layer.bias[i]*lamb  + torch.normal(0.0, sigma, size=(1,1))\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    iml_crit = nn.ReLU()\n",
    "    optimizer= torch.optim.SGD(model.parameters(), lr=0.001, momentum= 0.5)\n",
    "    epochs = 2000\n",
    "    lamda = 0.1\n",
    "    loss_over_time = []\n",
    "    test_loss_over_time = []\n",
    "\n",
    "    # Training Loop\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train).flatten()\n",
    "        loss = criterion(y_train, output) + lamda * iml_crit(torch.norm(lr_model(X_train, y_train)-output))\n",
    "        loss_over_time.append(loss.item())\n",
    "        test_loss_over_time.append(test_loss(model=model, X_test=X_test, y_test=y_test))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return test_loss_over_time[-1]\n",
    "\n",
    "def train_iml_multirun(K, i):\n",
    "    skf = sklearn.model_selection.KFold(n_splits=K)\n",
    "    skf.get_n_splits(X, y)\n",
    "    loss_test = []\n",
    "    cv_loss_test = Parallel(n_jobs=-1)(delayed(train_iml_cv)(train_index, test_index) for train_index, test_index in skf.split(X, y))\n",
    "    return cv_loss_test\n",
    "\n",
    "def train_mlp_cv(train_index, test_index):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model = ConcreteNN(9)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer= torch.optim.SGD(model.parameters(), lr=0.001, momentum= 0.5)\n",
    "    epochs = 2000\n",
    "    loss_over_time = []\n",
    "    test_loss_over_time = []\n",
    "\n",
    "    # Training Loop\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train).flatten()\n",
    "        loss = criterion(y_train, output)\n",
    "        loss_over_time.append(loss.item())\n",
    "        test_loss_over_time.append(test_loss(model=model, X_test=X_test, y_test=y_test))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return test_loss_over_time[-1]\n",
    "\n",
    "def train_mlp_multirun(K, i):\n",
    "    skf = sklearn.model_selection.KFold(n_splits=K)\n",
    "    skf.get_n_splits(X, y)\n",
    "    loss_test = []\n",
    "    cv_loss_test = Parallel(n_jobs=-1)(delayed(train_mlp_cv)(train_index, test_index) for train_index, test_index in skf.split(X, y))\n",
    "    return cv_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cement (component 1)(kg in a m^3 mixture)</th>\n",
       "      <th>Blast Furnace Slag (component 2)(kg in a m^3 mixture)</th>\n",
       "      <th>Fly Ash (component 3)(kg in a m^3 mixture)</th>\n",
       "      <th>Water  (component 4)(kg in a m^3 mixture)</th>\n",
       "      <th>Superplasticizer (component 5)(kg in a m^3 mixture)</th>\n",
       "      <th>Coarse Aggregate  (component 6)(kg in a m^3 mixture)</th>\n",
       "      <th>Fine Aggregate (component 7)(kg in a m^3 mixture)</th>\n",
       "      <th>Concrete compressive strength(MPa, megapascals)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>79.986111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>676.0</td>\n",
       "      <td>61.887366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>380.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>36.447770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>266.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>45.854291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>475.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>39.289790</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Cement (component 1)(kg in a m^3 mixture)  \\\n",
       "0                                      540.0   \n",
       "1                                      540.0   \n",
       "7                                      380.0   \n",
       "8                                      266.0   \n",
       "9                                      475.0   \n",
       "\n",
       "   Blast Furnace Slag (component 2)(kg in a m^3 mixture)  \\\n",
       "0                                                0.0       \n",
       "1                                                0.0       \n",
       "7                                               95.0       \n",
       "8                                              114.0       \n",
       "9                                                0.0       \n",
       "\n",
       "   Fly Ash (component 3)(kg in a m^3 mixture)  \\\n",
       "0                                         0.0   \n",
       "1                                         0.0   \n",
       "7                                         0.0   \n",
       "8                                         0.0   \n",
       "9                                         0.0   \n",
       "\n",
       "   Water  (component 4)(kg in a m^3 mixture)  \\\n",
       "0                                      162.0   \n",
       "1                                      162.0   \n",
       "7                                      228.0   \n",
       "8                                      228.0   \n",
       "9                                      228.0   \n",
       "\n",
       "   Superplasticizer (component 5)(kg in a m^3 mixture)  \\\n",
       "0                                                2.5     \n",
       "1                                                2.5     \n",
       "7                                                0.0     \n",
       "8                                                0.0     \n",
       "9                                                0.0     \n",
       "\n",
       "   Coarse Aggregate  (component 6)(kg in a m^3 mixture)  \\\n",
       "0                                             1040.0      \n",
       "1                                             1055.0      \n",
       "7                                              932.0      \n",
       "8                                              932.0      \n",
       "9                                              932.0      \n",
       "\n",
       "   Fine Aggregate (component 7)(kg in a m^3 mixture)  \\\n",
       "0                                              676.0   \n",
       "1                                              676.0   \n",
       "7                                              594.0   \n",
       "8                                              670.0   \n",
       "9                                              594.0   \n",
       "\n",
       "   Concrete compressive strength(MPa, megapascals)   \n",
       "0                                         79.986111  \n",
       "1                                         61.887366  \n",
       "7                                         36.447770  \n",
       "8                                         45.854291  \n",
       "9                                         39.289790  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import big dataset (n=1000) for warmstarting\n",
    "pretrain_data = pd.read_excel(\"Concrete_Data.xls\")\n",
    "pretrain_data.head()\n",
    "filter = pretrain_data['Age (day)'].isin([28])\n",
    "pretrain_data[filter]\n",
    "pretrain_data = pd.read_excel(\"Concrete_Data.xls\")\n",
    "filter = pretrain_data['Age (day)'].isin([28])\n",
    "pretrain_data = pretrain_data[filter]\n",
    "del pretrain_data['Age (day)']\n",
    "X = torch.FloatTensor(pretrain_data.iloc[:, 0:7].values)\n",
    "y = torch.FloatTensor(pretrain_data.iloc[:, 7].values)\n",
    "x_min = X.min(dim=0)[0]\n",
    "x_max = X.max(dim=0)[0]\n",
    "X = (X - x_min) / (x_max - x_min) # minmax scaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.76, random_state=1)\n",
    "pretrain_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train pretrain model\n",
    "start_model = ConcreteNN(7)\n",
    "loss_over_time, test_loss_over_time = train(start_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import small dataset (n=103)\n",
    "data = pd.read_csv(\"slump_test.csv\")\n",
    "\n",
    "X = torch.FloatTensor(data.iloc[:, 1:10].values)\n",
    "y = torch.FloatTensor(data.iloc[:, 10].values)\n",
    "x_min = X.min(dim=0)[0]\n",
    "x_max = X.max(dim=0)[0]\n",
    "X = (X - x_min) / (x_max - x_min) # min max scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 10\n",
    "multiruns = 50\n",
    "loss = Parallel(n_jobs=-1)(delayed(train_multirun)(multiruns, i) for i in range(kfold))\n",
    "loss_iml = Parallel(n_jobs=-1)(delayed(train_iml_multirun)(multiruns, i) for i in range(kfold))\n",
    "loss_mlp = Parallel(n_jobs=-1)(delayed(train_mlp_multirun)(multiruns, i) for i in range(kfold))\n",
    "\n",
    "hf = h5py.File('warmstarting.h5', 'w')\n",
    "hf.create_dataset('Loss Vanilla', data=loss)\n",
    "hf.create_dataset('Loss IML', data=loss_iml)\n",
    "hf.create_dataset('Loss Chen', data=loss_mlp)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.628148223392666\n",
      "6.249178976211697\n",
      "6.808675774153322\n"
     ]
    }
   ],
   "source": [
    "hf = h5py.File('warmstarting.h5', 'r')\n",
    "loss_va = np.array(hf.get('Loss Vanilla'))\n",
    "loss_iml = np.array(hf.get('Loss IML'))\n",
    "loss_mlp = np.array(hf.get('Loss Chen'))\n",
    "print(loss_mlp.mean())\n",
    "print(loss_va.mean())\n",
    "print(loss_iml.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('AutoML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c987f447b3d8b7fbc1b90bcfc8f9402522a4a53d74324611693e3c189ce101c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataset_welch import create_dataset\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True\n",
    "    })\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression, mutual_info_regression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import ARDRegression, LinearRegression, BayesianRidge\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "taguchi_array = np.array(loadmat('taguchiQ3N20.mat')['ans'])\n",
    "orthogonal_array = taguchi_array[:, np.random.permutation(taguchi_array.shape[1])]\n",
    "\n",
    "x_train, y_train = create_dataset(taguchi_array)\n",
    "x_test, y_test = create_dataset(orthogonal_array)\n",
    "x_test, y_test = x_test[:20, :], y_test[:20] # reduce test size\n",
    "\n",
    "x = np.array([i for i in range(20)])\n",
    "\n",
    "def get_model_results(idx, X, y, x_test, y_test):\n",
    "\n",
    "    olr = LinearRegression().fit(X[:, idx], y)\n",
    "    brr = BayesianRidge(compute_score=True, n_iter=30).fit(X[:, idx], y)\n",
    "    ard = ARDRegression(compute_score=True, n_iter=30).fit(X[:, idx], y)\n",
    "\n",
    "    models = [olr, brr, ard]\n",
    "\n",
    "    rmse_models = []\n",
    "    r2_models = []\n",
    "\n",
    "    for model in models:\n",
    "        y_pred = model.predict(x_test[:, idx])\n",
    "        test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        test_r2 = BestFitRate(y_test, y_pred)\n",
    "        rmse_models.append(test_rmse)\n",
    "        r2_models.append(test_r2)\n",
    "    print(['LR', 'BR', 'ARD'])\n",
    "    print(r2_models)\n",
    "    return rmse_models, r2_models\n",
    "\n",
    "def BestFitRate(y_target,y_est):\n",
    "    \n",
    "    if len(y_target)==len(y_est):\n",
    "        N = len(y_target)\n",
    "    else:\n",
    "        print(\"Dimensions don't match!\")\n",
    "        return None\n",
    "    \n",
    "    y_target = np.array(y_target).reshape((N,-1))\n",
    "    y_est = np.array(y_est).reshape((N,-1))\n",
    "    \n",
    "    y_target = y_target.astype(float)\n",
    "    y_est = y_est.astype(float)\n",
    "    \n",
    "    \n",
    "    e_pred = np.linalg.norm(y_target-y_est,axis=1)\n",
    "    e_mean = np.linalg.norm(y_target-np.mean(y_target),axis=1)\n",
    "    \n",
    "    BFR = 1-sum(e_pred**2) / sum(e_mean**2) \n",
    "    \n",
    "    BFR = BFR*100\n",
    "    \n",
    "    if BFR<0:\n",
    "        BFR = 0\n",
    "        \n",
    "    return BFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureImportanceEnsemble:\n",
    "    def __init__(self, x_train, y_train, n_features_to_select=3):\n",
    "\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.n_features_to_select = n_features_to_select\n",
    "\n",
    "        self.alpha = 0.1 # for lasso, ridge, elnet\n",
    "\n",
    "        self.fi1 = self.loo(self.get_dt)\n",
    "        self.fi2 = self.loo(self.get_f_test)\n",
    "        self.fi3 = self.loo(self.get_p_vals)\n",
    "        self.fi4 = self.loo(self.get_lasso_fi)\n",
    "        self.fi5 = self.loo(self.get_ridge_fi)\n",
    "        self.fi6 = self.loo(self.get_elnet_fi)\n",
    "        self.fi7 = self.loo(self.get_ard_precision)\n",
    "        self.fi8 = self.loo(self.get_rf_fi)\n",
    "        self.fi9 = self.loo(self.get_lr_fi)\n",
    "        self.fi10 = self.loo(self.get_xgboost_fi)\n",
    "\n",
    "        self.fi11 = np.random.uniform(0, 1, size=(1, 20)) # expert knowledge\n",
    "        \n",
    "        self.inner_ensemble_results = self.ensemble_inner_decision_process()\n",
    "        self.outer_ensemble_results = np.argsort(-self.inner_ensemble_results.mean(axis=0))\n",
    "        self.inner_ensemble_features = self.fi_single_choosen_features()\n",
    "        self.outer_ensemble_features = self.ensemble_outer_decision_process()\n",
    "        #self.ensemble = self.get_ensemble_results()\n",
    "\n",
    "    def loo(self, fi_model_function):\n",
    "        # insert feature importance algorithm\n",
    "        # do loo\n",
    "        # return feature importance of every run\n",
    "        x_train = self.x_train\n",
    "        y_train = self.y_train\n",
    "\n",
    "        loo = LeaveOneOut()\n",
    "\n",
    "        fis = []\n",
    "        for train, test in loo.split(x_train, y_train):\n",
    "            fis.append(fi_model_function(x_train[train], y_train[train]))\n",
    "\n",
    "        fis = np.vstack(fis)\n",
    "        return fis \n",
    "\n",
    "    def get_dt(self, x_train, y_train): # Gini Importance from DecisionTreeRegressior\n",
    "        model = DecisionTreeRegressor()\n",
    "        model.fit(x_train, y_train)\n",
    "        return model.feature_importances_\n",
    "\n",
    "    def get_f_test(self, x_train, y_train):\n",
    "        f_test, p_values = f_regression(x_train, y_train)\n",
    "        f_test /= np.max(f_test)\n",
    "        return f_test\n",
    "\n",
    "    def get_p_vals(self, x_train, y_train):\n",
    "        f_test, p_values = f_regression(x_train, y_train)\n",
    "        p_values = -np.log10(p_values)\n",
    "        p_values /= p_values.max()\n",
    "        return p_values\n",
    "\n",
    "    def get_lasso_fi(self, x_train, y_train):\n",
    "        lasso = Lasso(alpha=self.alpha, fit_intercept=False).fit(x_train, y_train)\n",
    "        lasso_importance = lasso.coef_\n",
    "        lasso_importance /= np.max(lasso_importance)\n",
    "        return lasso_importance\n",
    "\n",
    "    def get_ridge_fi(self, x_train, y_train):\n",
    "        ridge = Ridge(alpha=self.alpha, fit_intercept=False).fit(x_train, y_train)\n",
    "        ridge_importance = ridge.coef_\n",
    "        ridge_importance /= np.max(ridge_importance)\n",
    "        return ridge_importance\n",
    "    \n",
    "    def get_elnet_fi(self, x_train, y_train):\n",
    "        el_net = ElasticNet(alpha=self.alpha, fit_intercept=False).fit(x_train, y_train)\n",
    "        el_net_importance = el_net.coef_\n",
    "        el_net_importance /= np.max(el_net_importance)\n",
    "        return el_net_importance\n",
    "\n",
    "    def get_ard_precision(self, x_train, y_train):\n",
    "        ard = ARDRegression(compute_score=True, n_iter=30).fit(x_train, y_train)\n",
    "        prec = 1/ard.lambda_\n",
    "        prec_minmax = (prec - prec.min())/(prec.max()-prec.min())\n",
    "        return prec_minmax\n",
    "\n",
    "    def get_rf_fi(self, x_train, y_train):\n",
    "        rf = RandomForestRegressor()\n",
    "        rf.fit(x_train, y_train)\n",
    "        return rf.feature_importances_\n",
    "    \n",
    "    def get_lr_fi(self, x_train, y_train):\n",
    "        lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
    "        lr_importance = lr.coef_\n",
    "        lr_importance /= np.max(lr_importance)\n",
    "        return lr_importance\n",
    "\n",
    "    def get_xgboost_fi(self, x_train, y_train):\n",
    "        xgbr = xgb.XGBRegressor(verbosity=0) \n",
    "        xgbr.fit(x_train, y_train)\n",
    "        return xgbr.feature_importances_\n",
    "        \n",
    "    def ensemble_inner_decision_process(self):\n",
    "        fi1 = self.fi1\n",
    "        fi2 = self.fi2\n",
    "        fi3 = self.fi3\n",
    "        fi4 = self.fi4\n",
    "        fi5 = self.fi5\n",
    "        fi6 = self.fi6\n",
    "        fi7 = self.fi7\n",
    "        fi8 = self.fi8\n",
    "        fi9 = self.fi9\n",
    "        fi10 = self.fi10\n",
    "        fi11 = self.fi11\n",
    "        \n",
    "        fi_list = [fi1, fi2, fi3, fi4, fi5, fi6, fi7, fi8, fi9, fi10, fi11]\n",
    "        mean_values = []\n",
    "        for fi in fi_list:\n",
    "            mean_values.append(fi.mean(axis=0))\n",
    "        mean_values = np.vstack(mean_values)\n",
    "        return mean_values\n",
    "    \n",
    "    def ensemble_outer_decision_process(self):\n",
    "        outer_ensemble_results = self.inner_ensemble_results.mean(axis=0)\n",
    "        n_features_to_select = self.n_features_to_select\n",
    "\n",
    "        choosen_features = np.argpartition(outer_ensemble_results, -n_features_to_select)[-n_features_to_select:] # get the n_features_to_select largest elements\n",
    "        return choosen_features\n",
    "\n",
    "    def fi_single_choosen_features(self):\n",
    "        n_features_to_select = self.n_features_to_select\n",
    "        fi_single_choosen_features = []\n",
    "        for res in self.inner_ensemble_results:\n",
    "            choosen_features = np.argpartition(res, -n_features_to_select)[-n_features_to_select:]\n",
    "\n",
    "            fi_single_choosen_features.append(choosen_features)\n",
    "        fi_single_choosen_features = np.vstack(fi_single_choosen_features)\n",
    "        return fi_single_choosen_features\n",
    "\n",
    "    def get_idx(self, fi):\n",
    "        return np.argsort(-fi.mean(axis=0)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.724e+02, tolerance: 1.422e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.765e+02, tolerance: 1.467e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.724e+02, tolerance: 1.480e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.737e+02, tolerance: 1.478e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.768e+02, tolerance: 1.483e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.737e+02, tolerance: 1.429e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.724e+02, tolerance: 1.476e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.765e+02, tolerance: 1.440e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.724e+02, tolerance: 1.318e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.734e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+02, tolerance: 1.474e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+02, tolerance: 1.475e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.734e+02, tolerance: 1.444e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+02, tolerance: 1.405e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+02, tolerance: 1.477e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.736e+02, tolerance: 1.474e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+02, tolerance: 1.436e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.765e+02, tolerance: 1.416e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+02, tolerance: 1.480e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.679e+02, tolerance: 1.393e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.764e+02, tolerance: 1.483e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.764e+02, tolerance: 1.429e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.675e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.764e+02, tolerance: 1.465e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.764e+02, tolerance: 1.481e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.675e+02, tolerance: 1.452e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.758e+02, tolerance: 1.480e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.664e+02, tolerance: 1.394e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.758e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.771e+02, tolerance: 1.479e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.702e+02, tolerance: 1.459e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.771e+02, tolerance: 1.470e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.765e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.683e+02, tolerance: 1.420e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.765e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.771e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.745e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.746e+02, tolerance: 1.476e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+02, tolerance: 1.470e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.734e+02, tolerance: 1.478e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.736e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.757e+02, tolerance: 1.437e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.716e+02, tolerance: 1.452e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.719e+02, tolerance: 1.475e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+02, tolerance: 1.482e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.733e+02, tolerance: 1.473e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.775e+02, tolerance: 1.483e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.775e+02, tolerance: 1.478e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+02, tolerance: 1.446e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.775e+02, tolerance: 1.474e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.775e+02, tolerance: 1.481e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.747e+02, tolerance: 1.482e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+02, tolerance: 1.480e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.733e+02, tolerance: 1.482e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.776e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.775e+02, tolerance: 1.466e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.746e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.775e+02, tolerance: 1.476e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.775e+02, tolerance: 1.483e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.720e+02, tolerance: 1.478e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.775e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.701e+02, tolerance: 1.471e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.771e+02, tolerance: 1.479e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.771e+02, tolerance: 1.472e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.655e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.756e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.756e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.678e+02, tolerance: 1.475e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.765e+02, tolerance: 1.482e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.764e+02, tolerance: 1.476e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+02, tolerance: 1.476e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.715e+02, tolerance: 1.484e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.756e+02, tolerance: 1.473e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.735e+02, tolerance: 1.459e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.733e+02, tolerance: 1.478e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.766e+02, tolerance: 1.483e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.746e+02, tolerance: 1.431e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.744e+02, tolerance: 1.462e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/tmp/ipykernel_19881/3745804635.py:90: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  lr = Lasso(alpha=0, fit_intercept=False).fit(x_train, y_train)\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/hoschangn/anaconda3/envs/AutoML/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.771e+02, tolerance: 1.483e-01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11, 18, 16, 14, 9, 7, 5, 15, 17, 6, 4, 19, 8, 13, 1, 2, 10, 0, 3, 12]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIEnsemble = FeatureImportanceEnsemble(x_train, y_train, n_features_to_select=3)\n",
    "\n",
    "features_prio_idx = FIEnsemble.outer_ensemble_results.tolist()\n",
    "features_prio_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 18, 14, 16, 0, 9, 7, 10, 1, 6, 3, 12, 17, 13, 4, 5, 15, 2, 8, 19]\n",
      "[11, 18, 14, 16, 7, 9, 4, 8, 2, 10, 1, 0, 3, 13, 5, 6, 19, 15, 17, 12]\n",
      "[11, 18, 14, 16, 7, 9, 4, 8, 2, 10, 1, 13, 5, 6, 17, 3, 0, 15, 19, 12]\n",
      "[11, 18, 14, 16, 7, 9, 4, 17, 15, 13, 12, 0, 8, 6, 5, 3, 2, 1, 10, 19]\n",
      "[11, 18, 14, 16, 7, 9, 4, 2, 1, 6, 0, 3, 19, 12, 15, 17, 5, 13, 10, 8]\n",
      "[11, 18, 14, 16, 7, 9, 4, 2, 17, 15, 13, 12, 0, 6, 5, 3, 1, 19, 10, 8]\n",
      "[11, 18, 14, 16, 7, 9, 4, 8, 17, 5, 10, 2, 1, 13, 6, 3, 15, 19, 12, 0]\n",
      "[11, 18, 14, 16, 0, 7, 9, 10, 19, 4, 5, 12, 6, 13, 1, 15, 17, 3, 8, 2]\n",
      "[11, 18, 14, 16, 7, 9, 4, 2, 1, 6, 0, 3, 19, 12, 15, 17, 5, 13, 10, 8]\n",
      "[11, 18, 16, 14, 12, 7, 0, 10, 6, 13, 1, 5, 2, 4, 15, 3, 17, 9, 8, 19]\n",
      "[5, 15, 17, 16, 6, 9, 19, 8, 13, 7, 1, 18, 2, 10, 11, 0, 14, 4, 3, 12]\n"
     ]
    }
   ],
   "source": [
    "print(FIEnsemble.get_idx(FIEnsemble.fi1))\n",
    "print(FIEnsemble.get_idx(FIEnsemble.fi2))\n",
    "print(FIEnsemble.get_idx(FIEnsemble.fi3))\n",
    "print(FIEnsemble.get_idx(FIEnsemble.fi4))\n",
    "print(FIEnsemble.get_idx(FIEnsemble.fi5))\n",
    "print(FIEnsemble.get_idx(FIEnsemble.fi6))\n",
    "print(FIEnsemble.get_idx(FIEnsemble.fi7))\n",
    "print(FIEnsemble.get_idx(FIEnsemble.fi8))\n",
    "print(FIEnsemble.get_idx(FIEnsemble.fi9))\n",
    "print(FIEnsemble.get_idx(FIEnsemble.fi10))\n",
    "print(FIEnsemble.get_idx(FIEnsemble.fi11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LR', 'BR', 'ARD']\n",
      "[42.64232053114354, 42.37783892056704, 42.37783892056706]\n",
      "['LR', 'BR', 'ARD']\n",
      "[65.53715508426622, 65.33461483574008, 65.33688059819292]\n",
      "['LR', 'BR', 'ARD']\n",
      "[54.795991775111474, 54.93567700501385, 55.63711153404014]\n",
      "['LR', 'BR', 'ARD']\n",
      "[47.88109924687913, 48.437089432623836, 49.24627673348033]\n",
      "['LR', 'BR', 'ARD']\n",
      "[43.87596323736385, 44.51743499530302, 45.30695826607012]\n",
      "['LR', 'BR', 'ARD']\n",
      "[42.14184530867545, 42.764836640532536, 43.405350392458075]\n",
      "['LR', 'BR', 'ARD']\n",
      "[42.15837079532111, 42.893210603657096, 43.40558368927548]\n",
      "['LR', 'BR', 'ARD']\n",
      "[42.158370795321055, 43.007139902406564, 43.405584414157204]\n",
      "['LR', 'BR', 'ARD']\n",
      "[42.18607365479311, 43.15071552443398, 43.40595783060005]\n",
      "['LR', 'BR', 'ARD']\n",
      "[42.062419341505255, 43.15042464436816, 43.40465889106269]\n",
      "['LR', 'BR', 'ARD']\n",
      "[38.620380689673716, 39.856423465377446, 40.47120070518473]\n",
      "['LR', 'BR', 'ARD']\n",
      "[38.620380689673716, 39.98455972175022, 40.471202859260046]\n",
      "['LR', 'BR', 'ARD']\n",
      "[38.49569006367346, 39.98908955371178, 40.46962019670332]\n",
      "['LR', 'BR', 'ARD']\n",
      "[38.51456575950607, 40.142937049954256, 40.46988230256492]\n",
      "['LR', 'BR', 'ARD']\n",
      "[38.52153619620245, 40.30051366174189, 40.470052973902625]\n",
      "['LR', 'BR', 'ARD']\n",
      "[38.44855017231632, 40.36962126254788, 40.470700388370986]\n",
      "['LR', 'BR', 'ARD']\n",
      "[38.739019989656974, 40.7715427345399, 40.47108856425907]\n",
      "['LR', 'BR', 'ARD']\n",
      "[38.73901998965701, 40.91790553587117, 40.47108824548695]\n",
      "['LR', 'BR', 'ARD']\n",
      "[38.73901998965697, 41.06790483050952, 40.471087935865945]\n"
     ]
    }
   ],
   "source": [
    "for i in range(19):\n",
    "    get_model_results(features_prio_idx[0:i+1], x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LR', 'BR', 'ARD']\n",
      "[42.64232053114354, 42.37783892056704, 42.37783892056706]\n",
      "['LR', 'BR', 'ARD']\n",
      "[65.53715508426622, 65.33461483574008, 65.33688059819292]\n",
      "['LR', 'BR', 'ARD']\n",
      "[64.12715568419327, 64.23535284312054, 64.44513769975768]\n",
      "['LR', 'BR', 'ARD']\n",
      "[47.88109924687917, 48.43708943262386, 49.24627673348034]\n",
      "['LR', 'BR', 'ARD']\n",
      "[47.881099246879145, 48.580543160114544, 49.2462768005135]\n",
      "['LR', 'BR', 'ARD']\n",
      "[43.87596323736381, 44.652517114047185, 45.30695827051671]\n",
      "['LR', 'BR', 'ARD']\n",
      "[42.141845308675464, 42.87634679630044, 43.40535039430503]\n",
      "['LR', 'BR', 'ARD']\n",
      "[42.43248784480155, 43.2650254070649, 43.406336491413946]\n",
      "['LR', 'BR', 'ARD']\n",
      "[42.452658931524645, 43.40727441410285, 43.406519807439636]\n",
      "['LR', 'BR', 'ARD']\n",
      "[42.328079339029344, 43.40483236428252, 43.405101133910094]\n",
      "['LR', 'BR', 'ARD']\n",
      "[42.32807933902934, 43.5254646268287, 43.40509884665946]\n",
      "['LR', 'BR', 'ARD']\n",
      "[42.32807933902932, 43.64873528609154, 43.40509657196196]\n",
      "['LR', 'BR', 'ARD']\n",
      "[42.356115299016054, 43.80199367234789, 43.407188812007135]\n",
      "['LR', 'BR', 'ARD']\n",
      "[42.3779518883126, 43.95346740153704, 43.40593693611109]\n",
      "['LR', 'BR', 'ARD']\n",
      "[38.92357618038127, 40.674902491032306, 40.47266156015057]\n",
      "['LR', 'BR', 'ARD']\n",
      "[38.93814007510701, 40.829801324966574, 40.47166299633299]\n",
      "['LR', 'BR', 'ARD']\n",
      "[38.93814007510704, 40.97277192970758, 40.471662917771376]\n",
      "['LR', 'BR', 'ARD']\n",
      "[38.86248924710331, 41.04419170112664, 40.47245111067305]\n",
      "['LR', 'BR', 'ARD']\n",
      "[38.73901998965699, 41.06790483050954, 40.47108793586596]\n"
     ]
    }
   ],
   "source": [
    "prio_idx = FIEnsemble.get_idx(FIEnsemble.fi1)\n",
    "for i in range(19):\n",
    "    get_model_results(prio_idx[0:i+1], x_train, y_train, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AutoML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:18) \n[GCC 10.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c987f447b3d8b7fbc1b90bcfc8f9402522a4a53d74324611693e3c189ce101c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDBihsTUSf0Z"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "!pip install --upgrade xlrd # important to upgrade to open xls file\n",
        "!pip install git+https://github.com/shukon/HpBandSter.git # probably not necessary, additional to autoPyTorch\n",
        "!pip install autoPyTorch "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-EERWasQfaZ"
      },
      "outputs": [],
      "source": [
        "# import needed packages\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "import sklearn.ensemble\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "#read dataset\n",
        "data = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/AutoML/Concrete_Data.xls', engine='xlrd')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJnhLNKZUNrF"
      },
      "outputs": [],
      "source": [
        "# Generate Training, Test and Validation Dataset\n",
        "X = data.iloc[:, 0:8].to_numpy()\n",
        "y = data.iloc[:, 8].to_numpy()\n",
        "\n",
        "print('Size of X: ', X.shape)\n",
        "\n",
        "X_train, X_vt, y_train, y_vt = sklearn.model_selection.train_test_split(X, y, train_size=0.7,random_state=1)\n",
        "X_test, X_val, y_test, y_val = sklearn.model_selection.train_test_split(X_vt, y_vt, train_size=0.5,random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xlTpT3lvNpOM"
      },
      "outputs": [],
      "source": [
        "# Plot data\n",
        "plt.figure(figsize=(18,9))\n",
        "for i, col in zip(range(8), data.columns):\n",
        "  plt.plot(X[:, i], label=col)\n",
        "plt.plot(y, label='y')\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(1.35, 0.75))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCPCAOn_Sz3Q"
      },
      "outputs": [],
      "source": [
        "#Korrelationskoeffizienten\n",
        "for i in range(8):\n",
        "  print(data.columns[i], ' Pearson-Korrelation: ', np.corrcoef(X_train[:, i], y_train)[0,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99iaqakM65sc"
      },
      "outputs": [],
      "source": [
        "# Generate Dataset Class for PyTorch, so we can easily use the dataset for batch learning\n",
        "class ConcreteDataset(Dataset):\n",
        "  def __init__(self, data, labels):\n",
        "    # takes input data and target labels with dtype numpy array and converts it to a FloatTensor\n",
        "    self.data = torch.from_numpy(data).float() # Input Data\n",
        "    self.labels = torch.from_numpy(labels).float() # Target Labels\n",
        "  def __len__(self):\n",
        "    # returns size of dataset\n",
        "    return len(self.labels)\n",
        "  def __getitem__(self, idx):\n",
        "    # returns a single data row with target label\n",
        "    X = self.data[idx, :]\n",
        "    y = self.labels[idx].view(-1)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kni5CzrN8XFx"
      },
      "outputs": [],
      "source": [
        "# Generate Dataloaders for Training and Validation Data and determine batch size\n",
        "training_data = ConcreteDataset(X_train, y_train)\n",
        "train_dataloader = DataLoader(training_data, batch_size = 10, shuffle=True)\n",
        "# It's called test data but it's used for validation while training\n",
        "test_data = ConcreteDataset(X_val, y_val)\n",
        "test_dataloader = DataLoader(test_data, batch_size = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2mNUKmTuxDk"
      },
      "outputs": [],
      "source": [
        "# check if dataloader works\n",
        "data, labels = next(iter(train_dataloader))\n",
        "labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZj1a1dgDWgr"
      },
      "outputs": [],
      "source": [
        "# R2 Score for Validation Dataset \n",
        "def test_r2():\n",
        "  model.eval()\n",
        "  data, labels = next(iter(test_dataloader))\n",
        "  data = data.to(device)\n",
        "  labels = labels.to(device)\n",
        "  outputs = model(data)\n",
        "\n",
        "  return sklearn.metrics.r2_score(labels.cpu().detach().numpy(), outputs.cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfSPKZvxueU0"
      },
      "outputs": [],
      "source": [
        "# tested this physical loss to get better results. didnt work.\n",
        "# ReLU hits as long as output for cement from neural network and target label of cement isn't similiar + weight term\n",
        "def phy_loss(output, target):\n",
        "  mse = nn.MSELoss()\n",
        "  phy1 = torch.abs(torch.sum(output[:, 0] - target[:, 0]))\n",
        "  ReLU = nn.ReLU()\n",
        "  loss = mse(output, target) + 0.01*ReLU(phy1)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4BiK9MVoOXrQ"
      },
      "outputs": [],
      "source": [
        "# Generate Multilayer-Perceptron with ReLU Activation Function. Use of Adam Optimizer and MSELoss.\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.linear1 = nn.Linear(8, 100)\n",
        "        self.linear2 = nn.Linear(100, 50)\n",
        "        self.linear3 = nn.Linear(50, 50)\n",
        "        self.linear4 = nn.Linear(50, 10)\n",
        "        self.linear5 = nn.Linear(10, 1)\n",
        "        self.af = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        x = self.af(self.linear1(x))\n",
        "        x = self.af(self.linear2(x))\n",
        "        x = self.af(self.linear3(x))\n",
        "        x = self.af(self.linear4(x))\n",
        "        x= self.linear5(x)\n",
        "        return x\n",
        "\n",
        "model = MLP().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.MSELoss()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Go9wjcxUL7Hm"
      },
      "outputs": [],
      "source": [
        "# Training of MLP. Termination Criterions: max 6000 epochs. 1000 seconds.\n",
        "start_time = time.time()\n",
        "n_epochs = 6000\n",
        "list_loss = []\n",
        "test_list_loss = []\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    for data, target in train_dataloader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    train_loss = train_loss/len(train_dataloader.dataset)\n",
        "    list_loss.append(train_loss)\n",
        "    r2_test = test_r2()\n",
        "    test_list_loss.append(r2_test)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\t R^2 Test: {}'.format(\n",
        "        epoch+1, \n",
        "        train_loss,\n",
        "        r2_test\n",
        "        ))\n",
        "    timer = time.time() - start_time\n",
        "    if timer >= 1000:\n",
        "      break\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "runs = [] # initialize list of all test data r2 scores\n"
      ],
      "metadata": {
        "id": "m7yTndnp_1eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get R2 Score of MLP for Test Data\n",
        "model.eval()\n",
        "X_test = X_test\n",
        "y_test = y_test\n",
        "outputs = model(X_test)\n",
        "\n",
        "run = sklearn.metrics.r2_score(y_test.cpu().detach().numpy(), outputs.cpu().detach().numpy())\n",
        "runs.append(run)"
      ],
      "metadata": {
        "id": "-yAgENG8sRYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print average R2 Score for MLP\n",
        "print(sum(runs)/len(runs))\n"
      ],
      "metadata": {
        "id": "kX7dttIG_f64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbyj6HA291LO"
      },
      "outputs": [],
      "source": [
        "# Plot Training Loss\n",
        "plt.figure()\n",
        "plt.plot(list_loss[500:])\n",
        "# Plot R2 Score of Validation Data\n",
        "plt.figure(figsize=(14, 9))\n",
        "plt.plot(test_list_loss[500:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK2njUdv1Zge"
      },
      "source": [
        "# NAS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Used NAS based on following example: https://automl.github.io/Auto-PyTorch/master/examples/20_basics/example_tabular_regression.html#sphx-glr-examples-20-basics-example-tabular-regression-py"
      ],
      "metadata": {
        "id": "JdXuVfaki7Aj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xTso8-F68VR"
      },
      "outputs": [],
      "source": [
        "# Generate Train and Test Data\n",
        "data = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/AutoML/Concrete_Data.xls', engine='xlrd')\n",
        "X = data.iloc[:, 0:8].to_numpy()\n",
        "y = data.iloc[:, 8].to_numpy()\n",
        "\n",
        "print('Size of X: ', X.shape)\n",
        "\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    sklearn.model_selection.train_test_split(X, y,random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBX_86yh5NnK"
      },
      "outputs": [],
      "source": [
        "# Import TabularRegressionTask from autoPyTorch\n",
        "from autoPyTorch.api.tabular_regression import TabularRegressionTask\n",
        "\n",
        "api = TabularRegressionTask()\n",
        "\n",
        "# .search() takes train and test data, asks for optimization metric and how long the algorithm should run.\n",
        "# set memory_limit=None to use complete memory instead of default 4096 MB, so that used algorithms dont crash.\n",
        "api.search(\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_test=X_test.copy(),\n",
        "    y_test=y_test.copy(),\n",
        "    optimize_metric='r2',\n",
        "    total_walltime_limit=1000,\n",
        "    func_eval_time_limit_secs=50,\n",
        "    memory_limit=None\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqSIoiPj6gib"
      },
      "outputs": [],
      "source": [
        "y_pred = api.predict(X_test)\n",
        "\n",
        "# Rescale the Neural Network predictions into the original target range\n",
        "score = api.score(y_pred, y_test)\n",
        "\n",
        "print(score)\n",
        "\n",
        "# Print the final ensemble built by AutoPyTorch\n",
        "print(api.show_models())\n",
        "\n",
        "# Print statistics from search\n",
        "print(api.sprint_statistics())\n",
        "\n",
        "# wie wird der validation score gemessen?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wie sehen die Netzwerkarchitekturen der AutoML LÃ¶sung aus?\n"
      ],
      "metadata": {
        "id": "tMpfsoLxEN8f"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MLP190422.ipynb",
      "provenance": [],
      "mount_file_id": "1G85QzxggSGJROq7Paso61ZDK7dtjUOMv",
      "authorship_tag": "ABX9TyOLcUxN/WNEUfTQmABoTmKB"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
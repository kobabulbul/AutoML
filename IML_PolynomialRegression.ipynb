{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import torch.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import small dataset (n=103)\n",
    "data = pd.read_csv(\"slump_test.csv\")\n",
    "\n",
    "X = torch.FloatTensor(data.iloc[:, 1:10].values)\n",
    "y = torch.FloatTensor(data.iloc[:, 10].values)\n",
    "x_min = X.min(dim=0)[0]\n",
    "x_max = X.max(dim=0)[0]\n",
    "X = (X - x_min) / (x_max - x_min) # min max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteNN(nn.Module):\n",
    "\n",
    "  def __init__(self, size):\n",
    "    super().__init__()\n",
    "    # layers\n",
    "    self.input_layer = nn.Linear(size, size)     \n",
    "    self.hidden_layer = nn.Linear(size, size)\n",
    "    self.output_layer = nn.Linear(size, 1) \n",
    "    self.activation = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.input_layer(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.hidden_layer(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.output_layer(x)\n",
    "    return x\n",
    "\n",
    "def test_loss(model, X_test, y_test):\n",
    "  model.eval()\n",
    "  output = model(X_test)\n",
    "  loss = sklearn.metrics.mean_squared_error(output.detach().numpy(), y_test.detach().numpy())\n",
    "  return loss.item()\n",
    "\n",
    "def pr_model(X_train, y_train):\n",
    "    degree = 3\n",
    "    polyreg = make_pipeline(PolynomialFeatures(degree),LinearRegression())\n",
    "    inputs_reg = X_train[:, [0, 3, 5, 6]]\n",
    "    polyreg.fit(inputs_reg, y_train)\n",
    "    return torch.FloatTensor(polyreg.predict(inputs_reg))\n",
    "\n",
    "def train_iml_cv(train_index, test_index):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    model = ConcreteNN(9)\n",
    "    criterion = nn.MSELoss()\n",
    "    iml_crit = nn.ReLU()\n",
    "    optimizer= torch.optim.SGD(model.parameters(), lr=0.001, momentum= 0.5)\n",
    "    epochs = 2000\n",
    "    lamda = 0.1\n",
    "    loss_over_time = []\n",
    "    test_loss_over_time = []\n",
    "\n",
    "    # Training Loop\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train).flatten()\n",
    "        loss = criterion(y_train, output) + lamda * iml_crit(torch.norm(pr_model(X_train, y_train)-output))\n",
    "        loss_over_time.append(loss.item())\n",
    "        test_loss_over_time.append(test_loss(model=model, X_test=X_test, y_test=y_test))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return test_loss_over_time[-1]\n",
    "\n",
    "def train_iml_multirun(K, i):\n",
    "    skf = sklearn.model_selection.KFold(n_splits=K)\n",
    "    skf.get_n_splits(X, y)\n",
    "    loss_test = []\n",
    "    cv_loss_test = Parallel(n_jobs=-1)(delayed(train_iml_cv)(train_index, test_index) for train_index, test_index in skf.split(X, y))\n",
    "    return cv_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 10\n",
    "multiruns = 50\n",
    "loss = Parallel(n_jobs=-1)(delayed(train_iml_multirun)(multiruns, i) for i in range(kfold))\n",
    "\n",
    "hf = h5py.File('polyreg_iml.h5', 'w')\n",
    "hf.create_dataset('Loss IML', data=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.452442795142531\n",
      "9.099921045333147\n"
     ]
    }
   ],
   "source": [
    "hf = h5py.File('polyreg_iml.h5', 'r')\n",
    "loss = np.array(hf.get('Loss IML'))\n",
    "loss_va = np.array(hf.get('Loss Vanilla'))\n",
    "print(loss_va.mean())\n",
    "print(loss.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('AutoML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c987f447b3d8b7fbc1b90bcfc8f9402522a4a53d74324611693e3c189ce101c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

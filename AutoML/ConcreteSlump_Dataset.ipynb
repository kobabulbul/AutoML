{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koba341/AutoML/blob/main/ConcreteSlump_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Concrete Slump Dataset\n",
        "\n",
        "Dataset is from following paper: \\\\\n",
        " Yeh, I-Cheng, \"Modeling slump flow of concrete using second-order regressions and artificial neural networks,\" Cement and Concrete Composites, Vol.29, No. 6, 474-480, 2007."
      ],
      "metadata": {
        "id": "H1XScKWIyVSB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5s05M-XJOw7"
      },
      "outputs": [],
      "source": [
        "# install libraries\n",
        "!pip install autoPyTorch \n",
        "!pip install gpytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5xMnvLvoLmv6"
      },
      "outputs": [],
      "source": [
        "# import needed packages\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "import sklearn.model_selection\n",
        "import sklearn.ensemble\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import gpytorch\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP_Lcd6fKLi1",
        "outputId": "1238eaa3-a1ac-4b2a-c0ab-a9f3ee751804"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of X:  (103, 6)\n",
            "Size of X_train:  (82, 6)\n",
            "Size of X_test:  (21, 6)\n",
            "   No  Cement   Slag  Fly ash  Water    SP  Coarse Aggr.  Fine Aggr.  \\\n",
            "0   1   273.0   82.0    105.0  210.0   9.0         904.0       680.0   \n",
            "1   2   163.0  149.0    191.0  180.0  12.0         843.0       746.0   \n",
            "2   3   162.0  148.0    191.0  179.0  16.0         840.0       743.0   \n",
            "3   4   162.0  148.0    190.0  179.0  19.0         838.0       741.0   \n",
            "4   5   154.0  112.0    144.0  220.0  10.0         923.0       658.0   \n",
            "\n",
            "   SLUMP(cm)  FLOW(cm)  Compressive Strength (28-day)(Mpa)  \n",
            "0       23.0      62.0                               34.99  \n",
            "1        0.0      20.0                               41.14  \n",
            "2        1.0      20.0                               41.81  \n",
            "3        3.0      21.5                               42.08  \n",
            "4       20.0      64.0                               26.82  \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([683.8, 841.1, 827. , 765. , 742.7, 818. , 688. , 790. , 883. ,\n",
              "       743. , 679. , 649.1, 685. , 650. , 710. , 656. , 761. , 778. ,\n",
              "       815. , 680. , 715.3, 749. , 757. , 670.5, 672. , 768. , 720. ,\n",
              "       647.1, 683. , 737. , 644.1, 778. , 686. , 651.8, 797. , 765. ,\n",
              "       785. , 658. , 640.9, 646. , 792. , 804. , 652.5, 741. , 774. ,\n",
              "       696. , 780.5, 892. , 799. , 680. , 641.4, 758. , 757. , 829. ,\n",
              "       725. , 853. , 644.1, 691. , 723. , 667.2, 730. , 757. , 655. ,\n",
              "       902. , 704. , 804. , 705. , 776. , 775. , 695. , 789. , 789. ,\n",
              "       746. , 684. , 790. , 789.2, 829. , 780. , 722. , 813. , 836. ,\n",
              "       729. ])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "#import the data set\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/AutoML/slump_test.csv\")\n",
        "X = data.iloc[:, 1:7].to_numpy()\n",
        "y = data.iloc[:, 7:10].to_numpy()\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    sklearn.model_selection.train_test_split(X, y, train_size=0.80,random_state=1) # create training and test dataset. \n",
        "\n",
        "# print some information\n",
        "print('Size of X: ', X.shape)\n",
        "print('Size of X_train: ', X_train.shape)\n",
        "print('Size of X_test: ', X_test.shape)\n",
        "print(data.head()) # regression problem y:= (SLUMP, FLOW, Compressive Strength)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AutoPyTorch"
      ],
      "metadata": {
        "id": "vEIEasL-woWq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78nQbizdcKyi"
      },
      "outputs": [],
      "source": [
        "# Import TabularRegressionTask from autoPyTorch\n",
        "# AutoPyTorch cant access the GPU... \n",
        "from autoPyTorch.api.tabular_regression import TabularRegressionTask\n",
        "\n",
        "api = TabularRegressionTask(ensemble_size=0) #ensemble_size = 0: If set to 0, no ensemble will be constructed\n",
        "\n",
        "api.search(\n",
        "    X_train=X_train,\n",
        "    y_train=y_train,\n",
        "    X_test=X_test.copy(),\n",
        "    y_test=y_test.copy(),\n",
        "    optimize_metric='r2',\n",
        "    total_walltime_limit=3600*5, # Train for 5 hours\n",
        "    func_eval_time_limit_secs=np.inf, # When set to None, this time will automatically be set to total_walltime_limit // 2 to allow enough time to fit at least 2 individual machine learning algorithms. Set to np.inf in case no time limit is desired.\n",
        "    memory_limit=None,\n",
        "    enable_traditional_pipeline=False, # If set to False, no traditional machine learning pipelines will be trained.\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oSgfnTCh903"
      },
      "outputs": [],
      "source": [
        "y_pred = api.predict(X_test)\n",
        "\n",
        "# Rescale the Neural Network predictions into the original target range\n",
        "score = api.score(y_pred, y_test)\n",
        "\n",
        "print(score)\n",
        "\n",
        "# Print the final ensemble built by AutoPyTorch\n",
        "print(api.show_models())\n",
        "\n",
        "# Print statistics from search\n",
        "print(api.sprint_statistics())\n",
        "\n",
        "# Print model architecture\n",
        "print(api.models_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbvq4iX8ZUXm"
      },
      "source": [
        "# MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "cs31siIuZV8D"
      },
      "outputs": [],
      "source": [
        "# Generate Dataset Class for PyTorch, so we can easily use the dataset for batch learning\n",
        "# we have to normalize and standardize data maybe to get better results. Not done yet.\n",
        "class ConcreteDataset(Dataset):\n",
        "  def __init__(self, data, labels):\n",
        "    # takes input data and target labels with dtype numpy array and converts it to a FloatTensor\n",
        "    self.data = torch.from_numpy(data).float() # Input Data\n",
        "    self.labels = torch.from_numpy(labels).float() # Target Labels\n",
        "  def __len__(self):\n",
        "    # returns size of dataset\n",
        "    return len(self.labels)\n",
        "  def __getitem__(self, idx):\n",
        "    # returns a single data row with target label\n",
        "    X = self.data[idx, :]\n",
        "    y = self.labels[idx].view(-1)\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "hHDZcBfFZccs"
      },
      "outputs": [],
      "source": [
        "# Generate Dataloaders for Training and Validation Data and determine batch size\n",
        "training_data = ConcreteDataset(X_train, y_train)\n",
        "train_dataloader = DataLoader(training_data, batch_size = X_train.shape[0], shuffle=False)\n",
        "# It's called test data but it's used for validation while training\n",
        "test_data = ConcreteDataset(X_test, y_test)\n",
        "test_dataloader = DataLoader(test_data, batch_size = X_test.shape[0])\n",
        "\n",
        "# R2 Score for Validation Dataset \n",
        "def test_r2():\n",
        "  model.eval()\n",
        "  data, labels = next(iter(test_dataloader))\n",
        "  data = data.to(device)\n",
        "  labels = labels.to(device)\n",
        "  outputs = model(data)\n",
        "\n",
        "  return sklearn.metrics.r2_score(labels.cpu().detach().numpy(), outputs.cpu().detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "dqm9XejTZmpV"
      },
      "outputs": [],
      "source": [
        "# Generate Multilayer-Perceptron with ReLU Activation Function. Use of Adam Optimizer and MSELoss.\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.linear1 = nn.Linear(6, 7)\n",
        "        self.linear2 = nn.Linear(7, 3)\n",
        "        self.af = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.af(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "model = MLP().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
        "criterion = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pd7LOREMZwq1"
      },
      "outputs": [],
      "source": [
        "# Training of MLP. Termination Criterions: stop training after 2000 iterations.\n",
        "start_time = time.time()\n",
        "n_epochs = 2000\n",
        "list_loss = []\n",
        "test_list_loss = []\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    # monitor training loss\n",
        "    train_loss = 0.0\n",
        "    for data, target in train_dataloader:\n",
        "        data = data.to(device)\n",
        "        target = target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()*data.size(0)\n",
        "        \n",
        "    train_loss = train_loss/len(train_dataloader.dataset)\n",
        "    list_loss.append(train_loss)\n",
        "    r2_test = test_r2()\n",
        "    test_list_loss.append(r2_test)\n",
        "    print('Epoch: {} \\tTraining Loss: {:.6f} \\t R^2 Test: {}'.format(\n",
        "        epoch+1, \n",
        "        train_loss,\n",
        "        r2_test\n",
        "        ))\n",
        "    timer = time.time() - start_time\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gaussprozessregression\n",
        "made with https://docs.gpytorch.ai/en/stable/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.html"
      ],
      "metadata": {
        "id": "J-bluMHawibd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w44oAgn-1Odp"
      },
      "outputs": [],
      "source": [
        "# Set up GPR Model\n",
        "class MultitaskGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super(MultitaskGPModel, self).__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.MultitaskMean(\n",
        "            gpytorch.means.ConstantMean(), num_tasks=3\n",
        "        )\n",
        "        self.covar_module = gpytorch.kernels.MultitaskKernel(\n",
        "            gpytorch.kernels.MaternKernel(ard_num_dims=6), num_tasks=3, rank=1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultitaskMultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    sklearn.model_selection.train_test_split(X, y, train_size=0.8, random_state=1)\n",
        "\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "y_train = torch.from_numpy(y_train).float()\n",
        "X_test = torch.from_numpy(X_test).float()\n",
        "y_test = torch.from_numpy(y_test).float()\n",
        "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=3)\n",
        "model = MultitaskGPModel(X_train, y_train, likelihood)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xjbpbdgu18Cf"
      },
      "outputs": [],
      "source": [
        "# Train the GPR\n",
        "training_iterations = 100000\n",
        "\n",
        "\n",
        "# Find optimal model hyperparameters\n",
        "model.train()\n",
        "likelihood.train()\n",
        "\n",
        "# Use the adam optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Includes GaussianLikelihood parameters\n",
        "\n",
        "# \"Loss\" for GPs - the marginal log likelihood\n",
        "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
        "\n",
        "for i in range(training_iterations):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train)\n",
        "    loss = -mll(output, y_train)\n",
        "    loss.backward()\n",
        "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
        "    optimizer.step()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfYr3YYLFVgj"
      },
      "outputs": [],
      "source": [
        "# Print R2 Value of GPR\n",
        "model.eval()\n",
        "output= model(X_test)\n",
        "\n",
        "r2 = sklearn.metrics.r2_score(y_test, output.mean.detach().numpy())\n",
        "print('R2: ', r2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "ConcreteSlumpTest_18052022.ipynb",
      "provenance": [],
      "mount_file_id": "1JhbvBgNEjtFZEiqzI8LaWTe1QujT94Ug",
      "authorship_tag": "ABX9TyPC0zkoTqGtArRRLGRrBLq9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

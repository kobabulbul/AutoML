{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import torch.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.model_selection\n",
    "plt.rcParams['figure.dpi'] = 200\n",
    "from joblib import Parallel, delayed\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcreteNN(nn.Module):\n",
    "\n",
    "  def __init__(self, size):\n",
    "    super().__init__()\n",
    "    # layers\n",
    "    self.input_layer = nn.Linear(size, size)     \n",
    "    self.hidden_layer = nn.Linear(size, size)\n",
    "    self.output_layer = nn.Linear(size, 1) \n",
    "    self.activation = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.input_layer(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.hidden_layer(x)\n",
    "    x = self.activation(x)\n",
    "    x = self.output_layer(x)\n",
    "    return x\n",
    "\n",
    "def test_loss(model, X_test, y_test):\n",
    "  model.eval()\n",
    "  output = model(X_test)\n",
    "  loss = sklearn.metrics.mean_squared_error(output.detach().numpy(), y_test.detach().numpy())\n",
    "  return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import small dataset (n=103)\n",
    "data = pd.read_csv(\"slump_test.csv\")\n",
    "\n",
    "X = torch.FloatTensor(data.iloc[:, 1:10].values)\n",
    "y = torch.FloatTensor(data.iloc[:, 10].values)\n",
    "x_min = X.min(dim=0)[0]\n",
    "x_max = X.max(dim=0)[0]\n",
    "X = (X - x_min) / (x_max - x_min) # min max scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv(train_index, test_index):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    LR = sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
    "    inputs_reg = X_train[:, [0, 3, 5, 6]]\n",
    "    LR.fit(inputs_reg, y_train)\n",
    "    init_weights = LR.coef_\n",
    "\n",
    "    model = ConcreteNN(9)\n",
    "\n",
    "    # warmstarting (shrinking and perturb): https://arxiv.org/pdf/1910.08475.pdf\n",
    "    # Most significantly, it allows us to\n",
    "    # quickly fit high-performing models in sequential environments without having to retrain from scratch.\n",
    "    # Separately, it offers a slight regularization benefit, which in combination with the first property\n",
    "    # sometimes allows shrink-perturb models to generalize even better than randomly-initialized models.\n",
    "    init_input = [0, 3, 5, 6]\n",
    "    lamb = 0.3\n",
    "    sigma = 0.001\n",
    "    with torch.no_grad():\n",
    "        for i, (init) in enumerate(init_input):\n",
    "            model.input_layer.weight[init][init] = torch.Tensor([init_weights[i]])\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer= torch.optim.SGD(model.parameters(), lr=0.001, momentum= 0.5)\n",
    "    epochs = 2000\n",
    "    loss_over_time = []\n",
    "    test_loss_over_time = []\n",
    "\n",
    "    # Training Loop\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train).flatten()\n",
    "        loss = criterion(y_train, output)\n",
    "        loss_over_time.append(loss.item())\n",
    "        test_loss_over_time.append(test_loss(model=model, X_test=X_test, y_test=y_test))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return test_loss_over_time[-1]\n",
    "\n",
    "def train_multirun(K, i):\n",
    "    skf = sklearn.model_selection.KFold(n_splits=K)\n",
    "    skf.get_n_splits(X, y)\n",
    "    loss_test = []\n",
    "    cv_loss_test = Parallel(n_jobs=-1)(delayed(train_cv)(train_index, test_index) for train_index, test_index in skf.split(X, y))\n",
    "    return cv_loss_test\n",
    "\n",
    "\n",
    "def train_sp_cv(train_index, test_index):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    LR = sklearn.linear_model.LinearRegression(fit_intercept=True)\n",
    "    inputs_reg = X_train[:, [0, 3, 5, 6]]\n",
    "    LR.fit(inputs_reg, y_train)\n",
    "    init_weights = LR.coef_\n",
    "\n",
    "    model = ConcreteNN(9)\n",
    "\n",
    "    # warmstarting (shrinking and perturb): https://arxiv.org/pdf/1910.08475.pdf\n",
    "    # Most significantly, it allows us to\n",
    "    # quickly fit high-performing models in sequential environments without having to retrain from scratch.\n",
    "    # Separately, it offers a slight regularization benefit, which in combination with the first property\n",
    "    # sometimes allows shrink-perturb models to generalize even better than randomly-initialized models.\n",
    "\n",
    "    init_input = [0, 3, 5, 6]\n",
    "    lamb = 0.3\n",
    "    sigma = 0.001\n",
    "    with torch.no_grad():\n",
    "        for i, (init) in enumerate(init_input):\n",
    "            model.input_layer.weight[init][init] = torch.Tensor([init_weights[i]]) * lamb  + torch.normal(0.0, sigma, size=(1,1))\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer= torch.optim.SGD(model.parameters(), lr=0.001, momentum= 0.5)\n",
    "    epochs = 2000\n",
    "    loss_over_time = []\n",
    "    test_loss_over_time = []\n",
    "\n",
    "    # Training Loop\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train).flatten()\n",
    "        loss = criterion(y_train, output)\n",
    "        loss_over_time.append(loss.item())\n",
    "        test_loss_over_time.append(test_loss(model=model, X_test=X_test, y_test=y_test))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return test_loss_over_time[-1]\n",
    "\n",
    "def train_sp_multirun(K, i):\n",
    "    skf = sklearn.model_selection.KFold(n_splits=K)\n",
    "    skf.get_n_splits(X, y)\n",
    "    loss_test = []\n",
    "    cv_loss_test = Parallel(n_jobs=-1)(delayed(train_sp_cv)(train_index, test_index) for train_index, test_index in skf.split(X, y))\n",
    "    return cv_loss_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = 10\n",
    "multiruns = 50\n",
    "loss_warm = Parallel(n_jobs=-1)(delayed(train_multirun)(multiruns, i) for i in range(kfold))\n",
    "loss_sp = Parallel(n_jobs=-1)(delayed(train_sp_multirun)(multiruns, i) for i in range(kfold))\n",
    "\n",
    "hf = h5py.File('warmstarting.h5', 'w')\n",
    "hf.create_dataset('Loss Warmstarting LR', data=loss_warm)\n",
    "hf.create_dataset('Loss Warmstarting LR SP', data=loss_sp)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['Loss Vanilla', 'Loss Warmstarting LR', 'Loss Warmstarting LR SP']>\n",
      "9.541884859018028\n",
      "8.817429725181311\n",
      "7.64127300927043\n"
     ]
    }
   ],
   "source": [
    "hf = h5py.File('warmstarting_lr.h5', 'r')\n",
    "print(hf.keys())\n",
    "loss_va = np.array(hf.get('Loss Vanilla'))\n",
    "loss_lr = np.array(hf.get('Loss Warmstarting LR'))\n",
    "loss_lrsp = np.array(hf.get('Loss Warmstarting LR SP'))\n",
    "\n",
    "print(loss_va.mean()) # Loss without anything\n",
    "print(loss_lr.mean()) # Loss Warmstarting\n",
    "print(loss_lrsp.mean()) # Loss Warmstarting with Shrinking and Perturbing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('AutoML')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c987f447b3d8b7fbc1b90bcfc8f9402522a4a53d74324611693e3c189ce101c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
